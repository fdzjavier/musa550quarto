[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Changing Zillow House Values in Newark, NJ",
    "section": "",
    "text": "The aim of this project was to extend the application of our housing prediction model to a new city: Newark, New Jersey. As the largest city in the state, Newark is strategically located with easy access to New York City through various transportation modes, making it an vital hub within New Jersey and the larger New York Metropolitan Area. Home to significant institutions like Rutgers University, the Prudential Center, and Newark Liberty International Airport, Newark also serves as a vital center for employment and opportunities.\nHowever, in recent years, the city has witnessed a surge in development interest, accompanied by concerns among residents regarding escalating rents and an influx of wealthier populations. Thus, it became imperative to undertake this project, delving into the current dynamics of housing sales (or a proxy thereof) and crafting predictive models—both linear and random forest—that could anticipate changes in the Zillow Home Value Index (ZHVI) based on diverse sociodemographic factors.\nDespite its limitations, this approach involved innovative techniques such as API utilization, ZHVI data aggregation from zip codes to census tracts, and comprehensive incorporation of crucial predictors at the census tract level. Moving forward, it’s crucial to consideri refining the model by accessing more granular ZHVI data, ideally at the census tract or individual home-sale level. This project highlights the need for further investigation into Newark’s changing real estate market and how it affects its residents, given the limited data available.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets referenced: Zip Codes: https://data.ci.newark.nj.us/dataset/new-jersey-zip-codes-polygon\nZillow Data: https://www.zillow.com/research/data/\nCensus: referenced through its API",
    "crumbs": [
      "Project Purpose"
    ]
  },
  {
    "objectID": "index.html#project-purpose",
    "href": "index.html#project-purpose",
    "title": "Predicting Changing Zillow House Values in Newark, NJ",
    "section": "",
    "text": "The aim of this project was to extend the application of our housing prediction model to a new city: Newark, New Jersey. As the largest city in the state, Newark is strategically located with easy access to New York City through various transportation modes, making it an vital hub within New Jersey and the larger New York Metropolitan Area. Home to significant institutions like Rutgers University, the Prudential Center, and Newark Liberty International Airport, Newark also serves as a vital center for employment and opportunities.\nHowever, in recent years, the city has witnessed a surge in development interest, accompanied by concerns among residents regarding escalating rents and an influx of wealthier populations. Thus, it became imperative to undertake this project, delving into the current dynamics of housing sales (or a proxy thereof) and crafting predictive models—both linear and random forest—that could anticipate changes in the Zillow Home Value Index (ZHVI) based on diverse sociodemographic factors.\nDespite its limitations, this approach involved innovative techniques such as API utilization, ZHVI data aggregation from zip codes to census tracts, and comprehensive incorporation of crucial predictors at the census tract level. Moving forward, it’s crucial to consideri refining the model by accessing more granular ZHVI data, ideally at the census tract or individual home-sale level. This project highlights the need for further investigation into Newark’s changing real estate market and how it affects its residents, given the limited data available.\n\n\n\n\n\n\nImportant\n\n\n\nDatasets referenced: Zip Codes: https://data.ci.newark.nj.us/dataset/new-jersey-zip-codes-polygon\nZillow Data: https://www.zillow.com/research/data/\nCensus: referenced through its API",
    "crumbs": [
      "Project Purpose"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Methodology",
    "section": "",
    "text": "The methodology for this code involved several key steps.\n\n\nFirst, a statewide New Jersey ZIP code data (in json format) was imported using an API call and merged with Zillow data containing ZHVI change values from 2020 to 2022.\nNext, the Census API was utilized to obtain census tract information. The centroids of the census tract dataframes were then calculated to assign each census tract to its corresponding zip code, serving as a proxy for allocating ZHVI change values to the census tract level. Missing values were imputed with the mean ZHVI value.\n\n\n\nA final geodataframe was incorporated into both a linear regression and a random forest model. A 30-70 testing-training split was applied to both models, and their accuracies were compared for the testing and training data splits. Additionally, a cross-validation test with 3 folds was conducted to further assess model performance.",
    "crumbs": [
      "Methodology"
    ]
  },
  {
    "objectID": "about.html#data-prepartion",
    "href": "about.html#data-prepartion",
    "title": "Methodology",
    "section": "",
    "text": "First, a statewide New Jersey ZIP code data (in json format) was imported using an API call and merged with Zillow data containing ZHVI change values from 2020 to 2022.\nNext, the Census API was utilized to obtain census tract information. The centroids of the census tract dataframes were then calculated to assign each census tract to its corresponding zip code, serving as a proxy for allocating ZHVI change values to the census tract level. Missing values were imputed with the mean ZHVI value.",
    "crumbs": [
      "Methodology"
    ]
  },
  {
    "objectID": "about.html#model",
    "href": "about.html#model",
    "title": "Methodology",
    "section": "",
    "text": "A final geodataframe was incorporated into both a linear regression and a random forest model. A 30-70 testing-training split was applied to both models, and their accuracies were compared for the testing and training data splits. Additionally, a cross-validation test with 3 folds was conducted to further assess model performance.",
    "crumbs": [
      "Methodology"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nFor a deeper understanding of how the model was conducted, please reference the tabs below.",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Data Preparation",
    "section": "",
    "text": "Cleaning the ZIP Code data to show the ZHVI changes from 2020-2022\nStart by importing the packages we need:\n\nimport requests\nimport json\n\n# URL of the Zip Code JSON file\nurl = \"https://og-production-open-data-newarknj-892364687672.s3.amazonaws.com/resources/e801054d-2392-4413-af40-042e9bc986b9/njzctapolygon.geojson?Content-Type=application%2Fjson&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJJIENTAPKHZMIPXQ%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240515T045045Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f905d5c756751b7a2b8eb871f028ee3fddc6af4c1ca3688902e44b08008b3f53\"\n\n# Fetch the JSON data\nresponse = requests.get(url)\n\n\nimport geopandas as gpd\nimport folium\n\n# Load GeoJSON data into a GeoDataFrame\nzipcodes = gpd.read_file(\"njzctapolygon.geojson\")\n\n# Display the first few rows of the attribute table\nprint(zipcodes.head())\n\n# Create a map centered at the mean of geometry coordinates\nzipcodes_map = folium.Map(location=[zipcodes.geometry.centroid.y.mean(), zipcodes.geometry.centroid.x.mean()], zoom_start=10)\n\n# Add GeoJSON data to the map\nfolium.GeoJson(zipcodes).add_to(zipcodes_map)\n\n# Save the map as an HTML file\nzipcodes_map.save(\"zipcodes.html\")\n\n# Display the map\nzipcodes_map\n\n\nimport pandas as pd\nzillow_data = pd.read_csv(\"./data/zillowdata.csv\")\n\n\n# Convert 'RegionName' column to strings in order to add the leading \"0\" to the Zip Codes\nNewark_NJ_zillow_data['RegionName'] = Newark_NJ_zillow_data['RegionName'].astype(str)\n\n# Since Newark Zip Codes start with zero, python appears to be omitting them\n# Add leading zeros to Zip Codes\nNewark_NJ_zillow_data['RegionName'] = Newark_NJ_zillow_data['RegionName'].str.zfill(5)\n\n\ndef looks_like_a_date(column_name):\n    return column_name.startswith(\"20\")\n\nlist(\n    filter(looks_like_a_date, zillow_data.columns)\n)\n    \npd.melt(\n    Newark_NJ_zillow_data,\n    id_vars = [\"City\",\"State\",\"RegionName\"],\n    value_vars = list(\n        filter(looks_like_a_date, Newark_NJ_zillow_data.columns)\n    ),\n    var_name = \"Date\",\n    value_name = \"ZHVI\",\n)\n\n\nnewark_zhvi_tidy = Newark_NJ_zillow_data.melt(\n    id_vars=[\"City\", \"StateName\",\"RegionName\"],\n    value_vars=list(filter(looks_like_a_date, Newark_NJ_zillow_data.columns)), \n    var_name=\"Date\",\n    value_name=\"ZHVI\",\n)\n\n\ndef calculate_percent_increase(grouped_newark):\n    \"\"\"\n    Calculate the percent increase from 2020-03-31 to 2024-03-31.\n    \n    Note that `group_df` is the DataFrame for each group.\n    \"\"\"\n    \n    march20_sel = grouped_newark[\"Date\"] == \"2020-03-31\"\n    march22_sel = grouped_newark[\"Date\"] == \"2022-03-31\"\n    \n    # Get the data for each month (only 1 row, so squeeze it!)\n    march_2020 = grouped_newark.loc[march20_sel].squeeze()\n    march_2022 = grouped_newark.loc[march22_sel].squeeze()\n\n    # Columns to calculate percent change for\n    columns = [\"ZHVI\"]\n    \n    # Return the percent change for both columns\n    return 100 * (march_2022[columns] / march_2020[columns] - 1)\n\ngrouped_newark = newark_zhvi_tidy.groupby(\"RegionName\")\n\n\n#ZHVI Percent Increase for newark area \nresult_newark = grouped_newark.apply(calculate_percent_increase)\nresult_newark.sort_values(by=\"ZHVI\", ascending=True)\nprint(result_newark)\n\n#Avg change to zillow prices for Rest of newark\nresult_newark.mean()\nprint(\"Average change to ZHVI prices in Rest of newark:\", result_newark.mean())\n\n\n# merge zip code geometry with zillow zip code data\nmerged_zillow = zipcodes.merge(result_newark, left_on='GEOID10', right_on='RegionName', how='left')\n\n# Save the merged GeoDataFrame as a new GeoJSON file\nmerged_zillow.to_file(\"merged_data.geojson\", driver='GeoJSON')\n\n#drop unnecessary columns\nmerged_zillow = merged_zillow.drop(columns=['ZCTA5CE10', 'CLASSFP10', 'MTFCC10','FUNCSTAT10','ALAND10','AWATER10'])\n\n#rename columns\nmerged_zillow = merged_zillow.rename(columns={'INTPTLAT10': 'lat', 'INTPTLON10': 'lon', 'GEOID10': 'geoid'})\n\nmerged_zillow.head()\n\n\navg_prices_newark = merged_df['ZHVI'].mean()\nprint(\"Newark ZHVI Average Across Neighborhoods from 2020-2022:\",avg_prices_newark)\n\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Plot the GeoDataFrame\nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_df.plot(ax=ax, column='ZHVI', legend=True)\nplt.title('Avg ZHVI Change from 2020-2022 across Zip Codes')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n# Define the Census demographic variables\nvariables = [\n    \"NAME\",\n    \"B03002_001E\",\n    \"B03002_003E\", \n    \"B03002_004E\", \n    \"B03002_005E\", \n    \"B03002_006E\", \n    \"B03002_007E\", \n    \"B03002_008E\", \n    \"B03002_009E\", \n    \"B03002_012E\", \n]\n\n\nimport cenpy\nimport geopandas as gpd\nimport numpy as np\n\ncensustracts = gpd.read_file('./data/tracts2019.geojson')\ncensustracts = censustracts[['GEOID','NAMELSAD', 'Pop_Total', 'Pop_Total_Poverty','Pop_Edu_NoHS_Pct','geometry']]\ncensustracts = censustracts.rename(columns = {'GEOID':'geoid'})\ncensustracts = censustracts.rename(columns = {'NAMELSAD':'tract'})\ncensustracts = censustracts.rename(columns = {'Pop_Total':'totalpop'})\ncensustracts = censustracts.rename(columns = {'Pop_Total_Poverty':'poverty'})\ncensustracts = censustracts.rename(columns = {'Pop_Edu_NoHS_Pct':'noeducation'})\ncensustracts['geoid'] = censustracts['geoid'].astype(np.int64)\n\n# create a new column that creates the tract ID from the GEOID column\ncensustracts['tract'] = censustracts['geoid'].astype(str).str[5:11]\n\n\navailable = cenpy.explorer.available()\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nnewark_county_code = \"013\"\nnj_state_code = \"34\"\n\n\n# queries a table\nnewark_demographics = acs.query(\n    cols=variables,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": nj_state_code, \"county\": newark_county_code, \"tract\": \"*\"},\n)\n\n\nnewark_demographics = newark_demographics.rename(\n    columns={\n        \"B03002_001E\": \"Total\", \n        \"B03002_003E\": \"White\",  \n        \"B03002_004E\": \"Black\",  \n        \"B03002_005E\": \"AI/AN\", \n        \"B03002_006E\": \"Asian\",  \n        \"B03002_007E\": \"NH/PI\", \n        \"B03002_008E\": \"Other_\",  \n        \"B03002_009E\": \"Two Plus\",\n        \"B03002_012E\": \"Hispanic\",\n    }\n)\n\n\nnewark_demographics = pd.merge(newark_demographics, censustracts, on='tract', how='inner')\n\n\n# create percentages of race/ethnicity\nnewark_demographics['Black'] = pd.to_numeric(newark_demographics['Black'])\nnewark_demographics['Total'] = pd.to_numeric(newark_demographics['Total'])\nnewark_demographics['blk_percent'] = (newark_demographics['Black'] / newark_demographics['Total']) * 100\nnewark_demographics['White'] = pd.to_numeric(newark_demographics['White'])\nnewark_demographics['white_percent'] = (newark_demographics['White'] / newark_demographics['Total']) * 100\nnewark_demographics['Hispanic'] = pd.to_numeric(newark_demographics['Hispanic'])\nnewark_demographics['latino_percent'] = (newark_demographics['Hispanic'] / newark_demographics['Total']) * 100\nnewark_demographics['Asian'] = pd.to_numeric(newark_demographics['Asian'])\nnewark_demographics['asian_percent'] = (newark_demographics['Asian'] / newark_demographics['Total']) * 100\nnewark_demographics['AI/AN'] = pd.to_numeric(newark_demographics['AI/AN'])\nnewark_demographics['NH/PI'] = pd.to_numeric(newark_demographics['NH/PI'])\nnewark_demographics['Other_'] = pd.to_numeric(newark_demographics['Other_'])\nnewark_demographics['Two Plus'] = pd.to_numeric(newark_demographics['Two Plus'])\ncolumns_to_sum = [\"AI/AN\", \"NH/PI\", \"Other_\", \"Two Plus\"]\n\nnewark_demographics['other_percent'] = (newark_demographics[columns_to_sum].sum(axis=1) / newark_demographics['Total']) * 100\n\n\nnewark_demographics['geoid'] = newark_demographics['geoid'].astype('int64')\nnewark_demographics = newark_demographics.dropna()\nnewark_demographics.drop(columns=['state', 'county', 'block group'], axis=1, inplace=True)\n\n# convert into a gdf\nnewark_demographics = gpd.GeoDataFrame(newark_demographics)\n# Convert CRS to 4326\nnewark_demographics = newark_demographics.to_crs(merged_zillow.crs)\n\n\n# Calculate centroids of census tracts\nnewark_demographics['centroid'] = newark_demographics.geometry.centroid\n\n# Create a new column in census tract GeoDataFrame to store ZHVI values\nnewark_demographics['ZHVI'] = None\n\n# Loop through each census tract centroid\nfor idx, centroid in newark_demographics['centroid'].iteritems():\n    # Find the ZIP code polygon that contains the centroid\n    containing_zip = merged_zillow[merged_zillow.contains(centroid)].iloc[0]\n    # Get the ZHVI value of the containing ZIP code and assign it to the census tract\n    newark_demographics.at[idx, 'ZHVI'] = containing_zip['ZHVI']\n    \n# Drop unnecessary columns\nnewark_demographics = newark_demographics.drop(columns=['centroid'])\n\n# Display the updated census tract GeoDataFrame\nprint(newark_demographics)\n\n# Save the updated census tract GeoDataFrame as a new GeoJSON file\nnewark_demographics.to_file(\"census_tracts_with_ZHVI.geojson\", driver='GeoJSON')\n\n\n# create final csv\nregression_df = newark_demographics\nregression_df.to_csv('regression_df.csv', index=False)\n\nregression_gdf = gpd.GeoDataFrame(regression_df, geometry='geometry')\n\n# Handle missing values in the 'ZHVI' column\nmean_ZHVI = regression_gdf[\"ZHVI\"].mean()\nregression_gdf[\"ZHVI\"].fillna(mean_ZHVI, inplace=True)",
    "crumbs": [
      "Analysis",
      "Data Preparation"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Modeling",
    "section": "",
    "text": "This section describes how the linear regression, random forest model, and cross validation were developed & compared for accuracy and efficiency\n\n\nNameError: name 'np' is not defined",
    "crumbs": [
      "Analysis",
      "Modeling"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Results from the Prediction",
    "section": "",
    "text": "The map produced below compares the model’s predictions versus the actual observed change in ZHVI. Despite the model’s limitations, this approach involved innovative techniques such as API utilization, ZHVI data aggregation from zip codes to census tracts, and comprehensive incorporation of crucial predictors at the census tract level. Moving forward, it’s crucial to consideri refining the model by accessing more granular ZHVI data, ideally at the census tract or individual home-sale level. This project highlights the need for further investigation into Newark’s changing real estate market and how it affects its residents, given the limited data available.\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(ncols=2, figsize=(10, 10))\n\n# Predicted values\ndata.plot(\n    ax=axs[0],\n    column='prediction',\n    legend=True,\n    cmap='viridis',\n    linewidth=0.8,\n    edgecolor='0.8',\n    legend_kwds={'label': \"Predicted ZHVI\", 'orientation': \"horizontal\", 'shrink': 0.8}\n)\naxs[0].set_title(\"Predicted ZHVI\")\n\n# Actual values\ndata.plot(\n    ax=axs[1],\n    column='ZHVI',\n    legend=True,\n    cmap='viridis',\n    linewidth=0.8,\n    edgecolor='0.8',\n    legend_kwds={'orientation': \"horizontal\", 'shrink': 0.5}  # Adjust the 'shrink' parameter\n)\naxs[1].set_title(\"ZHVI\")\n\naxs[0].set_axis_off()\naxs[1].set_axis_off()\n\nplt.show()",
    "crumbs": [
      "Analysis",
      "Results from the Prediction"
    ]
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Data Preparation",
    "section": "",
    "text": "Code\nimport requests\nimport json\n\n# URL of the Zip Code JSON file\nurl = \"https://og-production-open-data-newarknj-892364687672.s3.amazonaws.com/resources/e801054d-2392-4413-af40-042e9bc986b9/njzctapolygon.geojson?Content-Type=application%2Fjson&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJJIENTAPKHZMIPXQ%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240515T045045Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f905d5c756751b7a2b8eb871f028ee3fddc6af4c1ca3688902e44b08008b3f53\"\n\n# Fetch the JSON data\nresponse = requests.get(url)\nThis is an example from the [Quarto documentation](https://quarto.org/docs/computations/python.html)\nthat shows how to mix executable Python code blocks into a markdown file in a \"Quarto markdown\" `.qmd` file.\n\n\n\n\nCode\nimport pandas as pd\nzillow_data = pd.read_csv(\"./data/zillowdata.csv\")\n\n\nMap illustrating the ZHVI change from 2020 to 2022\n\n\nCode\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Plot the GeoDataFrame\nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_df.plot(ax=ax, column='ZHVI', legend=True)\nplt.title('Avg ZHVI Change from 2020-2022 across Zip Codes')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html#zip-code",
    "href": "analysis/1-python-code-blocks.html#zip-code",
    "title": "Data Preparation",
    "section": "",
    "text": "Code\nimport requests\nimport json\n\n# URL of the Zip Code JSON file\nurl = \"https://og-production-open-data-newarknj-892364687672.s3.amazonaws.com/resources/e801054d-2392-4413-af40-042e9bc986b9/njzctapolygon.geojson?Content-Type=application%2Fjson&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJJIENTAPKHZMIPXQ%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240515T045045Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f905d5c756751b7a2b8eb871f028ee3fddc6af4c1ca3688902e44b08008b3f53\"\n\n# Fetch the JSON data\nresponse = requests.get(url)\nThis is an example from the [Quarto documentation](https://quarto.org/docs/computations/python.html)\nthat shows how to mix executable Python code blocks into a markdown file in a \"Quarto markdown\" `.qmd` file.\n\n\n\n\nCode\nimport pandas as pd\nzillow_data = pd.read_csv(\"./data/zillowdata.csv\")\n\n\nMap illustrating the ZHVI change from 2020 to 2022\n\n\nCode\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Plot the GeoDataFrame\nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_df.plot(ax=ax, column='ZHVI', legend=True)\nplt.title('Avg ZHVI Change from 2020-2022 across Zip Codes')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  }
]